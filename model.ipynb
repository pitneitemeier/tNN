{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import Operator as op\n",
    "import utils\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "g_dtype = torch.float32"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "lattice_sites = 4\n",
    "'''\n",
    "h1 = op.Sx(0) + op.Sx(1)\n",
    "h2 = op.Sz(0) + op.Sz(1)\n",
    "o = [(op.Sx(1),)]\n",
    "'''\n",
    "h2_range = [(3.,3.)]\n",
    "\n",
    "h1 = []\n",
    "for l in range(lattice_sites):\n",
    "  h1 = op.Sz(l) * op.Sz((l+1) % lattice_sites) + h1\n",
    "\n",
    "h2 = []\n",
    "for l in range(lattice_sites):\n",
    "  h2 = op.Sx(l) + h2\n",
    "\n",
    "o = []\n",
    "for l in range(lattice_sites):\n",
    "  o = op.Sx(l) * (1 / lattice_sites) + o\n",
    "\n",
    "h1_mat = utils.get_total_mat_els(h1, lattice_sites)\n",
    "h2_mat = utils.get_total_mat_els(h2, lattice_sites)\n",
    "o_mat = utils.get_total_mat_els(o, lattice_sites)\n",
    "\n",
    "h_map = utils.get_map(h1 + h2, lattice_sites)\n",
    "o_map = utils.get_map(o, lattice_sites)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "adding to sequence\n",
      "adding to sequence\n",
      "adding to sequence\n",
      "adding to sequence\n",
      "adding to sequence\n",
      "adding to sequence\n",
      "adding to sequence\n",
      "adding to sequence\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "#setting up the datamodule\n",
    "\n",
    "class train_data(Dataset):\n",
    "    def __init__(self, lattice_sites, h_mat_list, h_ranges_list, o_mat, t_min=0, t_max=1):\n",
    "        \n",
    "        assert(len(h_mat_list) - 1 == len(h_ranges_list))\n",
    "        #exact sampling for now\n",
    "        self.spins = utils.get_all_spin_configs(lattice_sites).type(g_dtype)\n",
    "\n",
    "        #saving ranges to generate alpha values each batch\n",
    "        self.t_min = t_min\n",
    "        self.t_max = t_max\n",
    "        self.h_ranges_list = h_ranges_list\n",
    "\n",
    "        #saving mat elements to pass to training loop with respective multipliers each loop\n",
    "        self.h_mat_list = h_mat_list\n",
    "        self.o_mat = o_mat\n",
    "        \n",
    "    def __len__(self):\n",
    "        #just setting 100000 as dataset size to get 100000 alphas for one epoch\n",
    "        return 100000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #creating the random alpha array of numspins with one value of (t, h_ext1, ..)\n",
    "        alpha_arr = torch.zeros((self.spins.shape[0], (len(self.h_mat_list))))\n",
    "        for i in range( len(self.h_mat_list) - 1 ):\n",
    "            max = self.h_ranges_list[i][1]\n",
    "            min = self.h_ranges_list[i][0] \n",
    "            alpha_arr[:, i+1] = ( (max - min) * torch.rand((1,1)) + min )\n",
    "        alpha_0 = alpha_arr.clone()\n",
    "        alpha_arr[:, 0] = ( ( self.t_max - self.t_min ) * torch.rand((1,1)) + self.t_min )\n",
    "        h_mat = self.h_mat_list[0]\n",
    "        for i in range(len(self.h_mat_list) - 1):\n",
    "            h_mat = torch.cat((h_mat, alpha_arr[0, i +1] * self.h_mat_list[i + 1]), dim=2)\n",
    "\n",
    "        return self.spins, alpha_arr, alpha_0, h_mat, self.o_mat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class val_data(Dataset):\n",
    "    def __init__(self, ED_data, ext_params : tuple, o_mat):\n",
    "        #exact sampling for now\n",
    "        self.spins = utils.get_all_spin_configs(lattice_sites).type(g_dtype)\n",
    "        #target Magnetizations from ED Code that \n",
    "        self.t_arr = torch.from_numpy(ED_data[:, 0]).unsqueeze(1)\n",
    "        self.O_target = torch.from_numpy(ED_data[:, 2])\n",
    "\n",
    "        #saving mat elements to pass to val loop\n",
    "        self.o_mat = o_mat\n",
    "        self.ext_params = torch.zeros((1, len(ext_params)))\n",
    "        for i in range(len(ext_params)):\n",
    "            self.ext_params[:, i] = ext_params[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        #just setting 100000 as dataset size to get 100000 alphas for one epoch\n",
    "        return self.t_arr.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t_arr = self.t_arr[index].repeat(self.spins.shape[0], 1)\n",
    "        ext_param = self.ext_params.repeat(self.spins.shape[0], 1)\n",
    "        return self.spins, torch.cat((t_arr, ext_param), dim=1), self.o_mat, self.O_target[index]\n",
    "\n",
    "\n",
    "ED_data = np.loadtxt('ED_data.csv', delimiter=',')\n",
    "#print(ED_data)\n",
    "val_data = val_data(ED_data, (3,), o_mat)\n",
    "val_dataloader = DataLoader(val_data, batch_size=len(val_data), num_workers=2)\n",
    "#print(val_data[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[-1., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.],\n",
      "        [-1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1., -1.],\n",
      "        [ 1., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1., -1.],\n",
      "        [ 1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.]]), tensor([[2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00],\n",
      "        [2.1169e-04, 3.0000e+00]], dtype=torch.float64), tensor([[[[1.0000+0.j, 1.0000+0.j, 1.0000+0.j, 0.2500+0.j],\n",
      "          [1.0000+0.j, 1.0000+0.j, 0.2500+0.j, 1.0000+0.j],\n",
      "          [1.0000+0.j, 0.2500+0.j, 1.0000+0.j, 1.0000+0.j],\n",
      "          [0.2500+0.j, 1.0000+0.j, 1.0000+0.j, 1.0000+0.j]],\n",
      "\n",
      "         [[1.0000+0.j, 1.0000+0.j, 1.0000+0.j, 0.2500+0.j],\n",
      "          [1.0000+0.j, 1.0000+0.j, 0.2500+0.j, 1.0000+0.j],\n",
      "          [1.0000+0.j, 0.2500+0.j, 1.0000+0.j, 1.0000+0.j],\n",
      "          [0.2500+0.j, 1.0000+0.j, 1.0000+0.j, 1.0000+0.j]]]]), tensor(1.0000, dtype=torch.float64))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data = train_data(lattice_sites, [h1_mat, h2_mat], h2_range, o_mat)\n",
    "train_dataloader = DataLoader(dataset=data, batch_size=2, num_workers=2)\n",
    "data_iter = iter(train_dataloader)\n",
    "\n",
    "spins, alpha, alpha_0, h_mat, o_mat_el = next(data_iter)\n",
    "print(alpha_0, '\\n', alpha)\n",
    "#print(alpha, '\\n' ,spins)#, '\\n' ,h_mat.shape, '\\n', o_mat_el.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0.0000, 3.7221],\n",
      "         [0.0000, 3.7221],\n",
      "         [0.0000, 3.7221],\n",
      "         [0.0000, 3.7221],\n",
      "         [0.0000, 3.7221],\n",
      "         [0.0000, 3.7221],\n",
      "         [0.0000, 3.7221],\n",
      "         [0.0000, 3.7221]],\n",
      "\n",
      "        [[0.0000, 3.7494],\n",
      "         [0.0000, 3.7494],\n",
      "         [0.0000, 3.7494],\n",
      "         [0.0000, 3.7494],\n",
      "         [0.0000, 3.7494],\n",
      "         [0.0000, 3.7494],\n",
      "         [0.0000, 3.7494],\n",
      "         [0.0000, 3.7494]]]) \n",
      " tensor([[[0.9389, 3.7221],\n",
      "         [0.9389, 3.7221],\n",
      "         [0.9389, 3.7221],\n",
      "         [0.9389, 3.7221],\n",
      "         [0.9389, 3.7221],\n",
      "         [0.9389, 3.7221],\n",
      "         [0.9389, 3.7221],\n",
      "         [0.9389, 3.7221]],\n",
      "\n",
      "        [[0.6008, 3.7494],\n",
      "         [0.6008, 3.7494],\n",
      "         [0.6008, 3.7494],\n",
      "         [0.6008, 3.7494],\n",
      "         [0.6008, 3.7494],\n",
      "         [0.6008, 3.7494],\n",
      "         [0.6008, 3.7494],\n",
      "         [0.6008, 3.7494]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, lattice_sites, h_map, o_init_map):\n",
    "    '''\n",
    "    Initializer for neural net\n",
    "    Parameters\n",
    "    __________\n",
    "    lattice_sites: int\n",
    "    h_mat_list: tensor, dtype=complex\n",
    "      the matrix elements of the hamiltonian \n",
    "      shape = (num_ext_params, )\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.lattice_net = nn.Sequential(\n",
    "      nn.Conv1d(1, 8, kernel_size=2, padding=1, padding_mode='circular'),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(8, 16, kernel_size=2, padding=1, padding_mode='circular'),\n",
    "      nn.Flatten(start_dim=1, end_dim=-1)\n",
    "    )\n",
    "    \n",
    "    self.tNN = nn.Sequential(\n",
    "      nn.Linear(2, 16),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(16, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 64)\n",
    "    )\n",
    "\n",
    "    self.psi = nn.Sequential(\n",
    "      nn.Linear( 64 + 16 * ( lattice_sites + 2 ), 128 ),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64,2)\n",
    "    )\n",
    "\n",
    "    self.h_map = h_map\n",
    "    self.o_init_map = o_init_map\n",
    "\n",
    "  def forward(self, spins, alpha):\n",
    "    #unsqueeze since circular padding needs tensor of dim 3\n",
    "    lat_out = self.lattice_net(spins.unsqueeze(1))\n",
    "    t_out = self.tNN(alpha)\n",
    "\n",
    "    rad_and_phase = (self.psi(torch.cat((lat_out, t_out), dim=1)))\n",
    "    psi = rad_and_phase[:, 0] * torch.exp( 1.j * rad_and_phase[:, 1] )\n",
    "    return psi\n",
    "    \n",
    "  def call_forward(self, spins, alpha):\n",
    "    '''\n",
    "    makes forward callable with (num_alpha_configs, num_spin_configs)\n",
    "    Parameters\n",
    "    __________\n",
    "    spins: tensor, dtype=float\n",
    "      tensor of input spins to wave function \n",
    "      shape = (num_spin_configs, num_alpha_configs, lattice_sites)\n",
    "    alpha: tensor, dtype=float\n",
    "      other inputs to hamiltonian e.g. (time, ext_param) \n",
    "      shape = (num_spin_configs, num_alpha_configs, num_inputs)\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    psi: tensor, dtype=complex\n",
    "      wave function for a combination of (spins, alpha) \n",
    "      size = (num_spin_configs, num_alpha_configs, 1)\n",
    "    '''\n",
    "    spin_shape = spins.shape\n",
    "    alpha_shape = alpha.shape\n",
    "    \n",
    "    spins = torch.flatten(spins, end_dim=-2)\n",
    "    alpha = torch.flatten(alpha, end_dim=-2)\n",
    "    \n",
    "    psi = self(spins, alpha)\n",
    "    return psi.reshape( spin_shape[0], spin_shape[1], 1)\n",
    "\n",
    "  def call_forward_sp(self, sprimes, alpha):\n",
    "    '''\n",
    "    makes forward callable with (num_alpha_configs, num_spin_configs, num_sprimes)\n",
    "    Parameters\n",
    "    __________\n",
    "    spins: tensor, dtype=float\n",
    "      tensor of input spins to wave function \n",
    "      shape = (num_spin_configs, num_alpha_configs, num_sprimes, lattice_sites)\n",
    "    alpha: tensor, dtype=float\n",
    "      other inputs to hamiltonian e.g. (time, ext_param) are broadcasted to s' shape\n",
    "      shape = (num_spin_configs, num_alpha_configs, num_inputs)\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    psi: tensor, dtype=complex\n",
    "      wave function for a combination of (spins, alpha) \n",
    "      size = (num_spin_configs, num_alpha_configs, num_sprimes, 1)\n",
    "    '''\n",
    "    sprimes_shape = sprimes.shape\n",
    "    alpha_shape = alpha.shape\n",
    "    \n",
    "    alpha = alpha.unsqueeze(2)\n",
    "    alpha = alpha.broadcast_to(alpha_shape[0], alpha_shape[1], sprimes_shape[2], alpha_shape[2])\n",
    "\n",
    "    sprimes = torch.flatten(sprimes, end_dim=-2)\n",
    "    alpha = torch.flatten(alpha, end_dim=-2)\n",
    "    \n",
    "    psi = self(sprimes, alpha)\n",
    "    return psi.reshape( sprimes_shape[0], sprimes_shape[1], sprimes_shape[2], 1)\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    spins, alpha, alpha_0, h_mat, o_mat = batch\n",
    "    alpha.requires_grad = True\n",
    "    #get psi(s, alpha)\n",
    "    psi_s = self.call_forward(spins, alpha)\n",
    "\n",
    "    #get s' and psi(s', alpha) for h\n",
    "    sp_h = utils.get_sp(spins, self.h_map)\n",
    "    psi_sp_h = self.call_forward_sp(sp_h, alpha)\n",
    "\n",
    "    #get s' and psi(s', alpha) for o at t=0\n",
    "    sp_o = utils.get_sp(spins, self.o_init_map)\n",
    "    psi_sp_o = self.call_forward_sp(sp_o, alpha_0)\n",
    "    psi_s_0 = self.call_forward(spins, alpha_0)\n",
    "    \n",
    "    #calc O_loc for h, o\n",
    "    h_loc = utils.calc_Oloc(psi_sp_h, h_mat, spins)\n",
    "    o_loc = utils.calc_Oloc(psi_sp_o, o_mat, spins)\n",
    "\n",
    "    #calc dt_psi(s, alpha)\n",
    "    dt_psi_s = utils.calc_dt_psi(psi_s, alpha)\n",
    "    #print(dt_psi_s.shape, psi_s.shape)\n",
    "\n",
    "    #calc loss\n",
    "    loss = utils.loss(dt_psi_s, h_loc, psi_s_0, o_loc)\n",
    "    return {'loss': loss}\n",
    "\n",
    "  def validation_step(self):\n",
    "    print('measuring observable')\n",
    "    \n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "model = Model(lattice_sites, h_map, o_map)\n",
    "print(model)\n",
    "#spins, alpha, _, _ = next(data_iter)\n",
    "#print(spins.shape, alpha.shape)\n",
    "#print(model.call_forward(spins, alpha))\n",
    "#print(model(spins, alpha))\n",
    "\n",
    "trainer = pl.Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, train_dataloader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | lattice_net | Sequential | 296   \n",
      "1 | tNN         | Sequential | 2.7 K \n",
      "2 | psi         | Sequential | 26.9 K\n",
      "-------------------------------------------\n",
      "29.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "29.9 K    Total params\n",
      "0.120     Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model(\n",
      "  (lattice_net): Sequential(\n",
      "    (0): Conv1d(1, 8, kernel_size=(2,), stride=(1,), padding=(1,), padding_mode=circular)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(8, 16, kernel_size=(2,), stride=(1,), padding=(1,), padding_mode=circular)\n",
      "    (3): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (tNN): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
      "  )\n",
      "  (psi): Sequential(\n",
      "    (0): Linear(in_features=144, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22ec06cc34804bef81cce36e3ea288dc"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}