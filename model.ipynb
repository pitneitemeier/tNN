{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import Operator as op\n",
    "import utils\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "g_dtype = torch.float32"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the Hamiltonian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "lattice_sites = 3\n",
    "h1 = op.Sx(0) + op.Sx(1)\n",
    "op.print_op_list(h1)\n",
    "h2 = op.Sz(0) + op.Sz(1)\n",
    "o = op.Sx(1) + []\n",
    "h1_mat = utils.get_total_mat_els(h1, lattice_sites)\n",
    "h2_mat = utils.get_total_mat_els(h2, lattice_sites)\n",
    "o_mat = utils.get_total_mat_els(o, lattice_sites)\n",
    "h2_range = [(3,4)]\n",
    "\n",
    "'''\n",
    "for l in range(lattice_sites):\n",
    "  hamiltonian = Sx(l)* (h) + Sz(l) * Sz((l+1) % lattice_sites) + hamiltonian\n",
    "print_op_list(hamiltonian)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "adding\n",
      "adding operator\n",
      "Hamiltonan = Sx_0 + Sx_1\n",
      "adding\n",
      "adding operator\n",
      "adding\n",
      "adding to sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor l in range(lattice_sites):\\n  hamiltonian = Sx(l)* (h) + Sz(l) * Sz((l+1) % lattice_sites) + hamiltonian\\nprint_op_list(hamiltonian)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "#setting up the datamodule\n",
    "\n",
    "class spin_data(Dataset):\n",
    "    def __init__(self, lattice_sites, h_mat_list, h_ranges_list, o_mat, t_min=0, t_max=1):\n",
    "        \n",
    "        assert(len(h_mat_list) - 1 == len(h_ranges_list))\n",
    "        #exact sampling for now\n",
    "        self.spins = utils.get_all_spin_configs(lattice_sites).type(g_dtype)\n",
    "\n",
    "        #saving ranges to generate alpha values each batch\n",
    "        self.t_min = t_min\n",
    "        self.t_max = t_max\n",
    "        self.h_ranges_list = h_ranges_list\n",
    "\n",
    "        #saving mat elements to pass to training loop with respective multipliers each loop\n",
    "        self.h_mat_list = h_mat_list\n",
    "        self.o_mat = o_mat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #creating the random alpha array of numspins with one value of (t, h_ext1, ..)\n",
    "        alpha_arr = torch.ones((self.spins.shape[0], (len(self.h_mat_list))))\n",
    "        alpha_arr[:, 0] = ( ( self.t_max - self.t_min ) * torch.rand((1,1)) + self.t_min )\n",
    "        for i in range( len(self.h_mat_list) - 1 ):\n",
    "            max = self.h_ranges_list[i][1]\n",
    "            min = self.h_ranges_list[i][0] \n",
    "            alpha_arr[:, i+1] = ( (max - min) * torch.rand((1,1)) + min )\n",
    "        return self.spins, alpha_arr, torch.cat(), self.o_mat\n",
    "\n",
    "    def cuda(self):\n",
    "        self.spins = self.spins.to(device)\n",
    "        self.alpha_arr = self.alpha_arr.to(device)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "data = spin_data(lattice_sites, [h1_mat, h2_mat], h2_range, o_mat)\n",
    "#data.cuda()\n",
    "dataloader = DataLoader(dataset=data, batch_size=2)\n",
    "data_iter = iter(dataloader)\n",
    "spins, alpha = next(data_iter)\n",
    "#spins, alpha = data[0]\n",
    "print(alpha, '\\n\\n' ,spins)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0.9498, 3.5746],\n",
      "         [0.9498, 3.5746],\n",
      "         [0.9498, 3.5746],\n",
      "         [0.9498, 3.5746],\n",
      "         [0.9498, 3.5746],\n",
      "         [0.9498, 3.5746],\n",
      "         [0.9498, 3.5746],\n",
      "         [0.9498, 3.5746]],\n",
      "\n",
      "        [[0.4487, 3.1379],\n",
      "         [0.4487, 3.1379],\n",
      "         [0.4487, 3.1379],\n",
      "         [0.4487, 3.1379],\n",
      "         [0.4487, 3.1379],\n",
      "         [0.4487, 3.1379],\n",
      "         [0.4487, 3.1379],\n",
      "         [0.4487, 3.1379]]]) \n",
      "\n",
      " tensor([[[-1., -1., -1.],\n",
      "         [ 1., -1., -1.],\n",
      "         [-1., -1.,  1.],\n",
      "         [-1.,  1., -1.],\n",
      "         [ 1.,  1., -1.],\n",
      "         [ 1., -1.,  1.],\n",
      "         [-1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1.],\n",
      "         [ 1., -1., -1.],\n",
      "         [-1., -1.,  1.],\n",
      "         [-1.,  1., -1.],\n",
      "         [ 1.,  1., -1.],\n",
      "         [ 1., -1.,  1.],\n",
      "         [-1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "a = torch.ones((4,2))\n",
    "a[:, 0] = ((2 - 1) * torch.rand((1,)) + 1)\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.8491, 1.0000],\n",
      "        [1.8491, 1.0000],\n",
      "        [1.8491, 1.0000],\n",
      "        [1.8491, 1.0000]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, lattice_sites, h_mat_list, h_map, o_init_mat, o_init_map):\n",
    "    '''\n",
    "    Initializer for neural net\n",
    "    Parameters\n",
    "    __________\n",
    "    lattice_sites: int\n",
    "    h_mat_list: tensor, dtype=complex\n",
    "      the matrix elements of the hamiltonian \n",
    "      shape = (num_ext_params, )\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.lattice_net = nn.Sequential(\n",
    "      nn.Conv1d(1, 8, kernel_size=2, padding=1, padding_mode='circular'),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(8, 16, kernel_size=2, padding=1, padding_mode='circular'),\n",
    "      nn.Flatten(start_dim=1, end_dim=-1)\n",
    "    )\n",
    "    \n",
    "    self.tNN = nn.Sequential(\n",
    "      nn.Linear(2, 16),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(16, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 64)\n",
    "    )\n",
    "\n",
    "    self.psi = nn.Sequential(\n",
    "      nn.Linear( 64 + 16 * ( lattice_sites + 2 ), 128 ),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64,2)\n",
    "    )\n",
    "\n",
    "    self.h_mat_list = h_mat_list\n",
    "    self.o_init_mat = o_init_mat\n",
    "    self.h_map = h_map\n",
    "    self.o_init_map = o_init_map\n",
    "\n",
    "  def forward(self, spins, alpha):\n",
    "    '''\n",
    "    Forward function of the neural net to calculate psi. uses same spin config for all alpha values\n",
    "    Parameters\n",
    "    __________\n",
    "    spins: tensor, dtype=float\n",
    "      tensor of input spins to wave function \n",
    "      shape = (num_spin_configs, lattice_sites)\n",
    "    alpha: tensor, dtype=float\n",
    "      other inputs to hamiltonian e.g. (time, ext_param) \n",
    "      shape = (num_alpha_configs, num_inputs)\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "    psi: tensor, dtype=complex\n",
    "      wave function for a combination of (spins, alpha) \n",
    "      size = (num_spin_configs, num_alpha_configs)\n",
    "    '''\n",
    "\n",
    "    spin_shape = spins.shape\n",
    "    alpha_shape = alpha.shape\n",
    "    \n",
    "    #unsqueeze since circular padding needs tensor of dim 3\n",
    "    spins = torch.flatten(spins, end_dim=-2).unsqueeze(1)\n",
    "    alpha = torch.flatten(alpha, end_dim=-2)\n",
    "\n",
    "    lat_out = self.lattice_net(spins)\n",
    "    t_out = self.tNN(alpha)\n",
    "\n",
    "    rad_and_phase = (self.psi(torch.cat((lat_out, t_out), dim=1)))\n",
    "    psi = rad_and_phase[:, 0] * torch.exp( 1.j * rad_and_phase[:, 1] )\n",
    "    return psi.reshape( spin_shape[0], spin_shape[1], 1)\n",
    "    \n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    spins, alpha = batch\n",
    "    #get psi(s, alpha)\n",
    "    psi = self(spins, alpha)\n",
    "    #get map and s' to the s from batch_idx\n",
    "\n",
    "    #get psi(s', alpha)\n",
    "\n",
    "    #calc dt_psi(s, alpha)\n",
    "\n",
    "    #get mat_els for all alphas\n",
    "\n",
    "    #get map, s' mat_els for O_loc_init\n",
    "\n",
    "    #calc loss\n",
    "    print('nyi')\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "model = Model(lattice_sites)\n",
    "print(model)\n",
    "#print(model(spins, alpha))\n",
    "\n",
    "#trainer = pl.Trainer(fast_dev_run=True)\n",
    "#trainer.fit(model, dataloader)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-a53fa7ac6cbc>, line 67)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-a53fa7ac6cbc>\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    spins, alpha =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "map = utils.get_map(hamiltonian, lattice_sites)\n",
    "map = map.to(device)\n",
    "print(\"map: \", map)\n",
    "\n",
    "mat_els = utils.get_total_mat_els(hamiltonian, lattice_sites)\n",
    "mat_els = mat_els.to(device)\n",
    "print(\"mat els: \", mat_els)\n",
    "\n",
    "#1.dim: Batch\n",
    "#2.dim: lattice sites\n",
    "#s_config = get_all_spin_configs(lattice_sites)\n",
    "s_config = torch.tensor([1,1,1]).reshape(1,3)\n",
    "s_config = (s_config.type(torch.float32)).to(device)\n",
    "print(\"spin config: \", s_config)\n",
    "\n",
    "start = timer()\n",
    "s_p = utils.get_sp(s_config, map)\n",
    "psi_s = model(s_config)\n",
    "psi_sp = model(s_p.reshape(-1, lattice_sites)).reshape(s_p.shape[0], s_p.shape[1])\n",
    "print(psi_sp.shape, psi_s.shape, s_config.shape)\n",
    "O_loc = utils.calc_Oloc(psi_sp, mat_els, s_config)\n",
    "end = timer()\n",
    "\n",
    "print(f\"time to calculate O_loc: {end - start:.2e}\") "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "map:  tensor([[[ 1,  1, -1],\n",
      "         [ 1,  1, -1]]], dtype=torch.int8)\n",
      "mat els:  tensor([[[[ 1.+0.j,  1.+0.j,  1.+0.j],\n",
      "          [ 1.+0.j,  1.+0.j,  0.+1.j]],\n",
      "\n",
      "         [[ 1.+0.j,  1.+0.j,  1.+0.j],\n",
      "          [ 1.+0.j, -1.+0.j, -0.-1.j]]]])\n",
      "spin config:  tensor([[1., 1., 1.]])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'alpha'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-48b6577b80c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0ms_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpsi_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mpsi_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlattice_sites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_sp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsi_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'alpha'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "spins = torch.full((2,5), 0.5)\n",
    "spins[1,:] /=4\n",
    "\n",
    "alpha = torch.full((4,2), 1)\n",
    "alpha[:, 1] = 2\n",
    "alpha[:2, : ]*=4\n",
    "spins_batched = torch.flatten(torch.broadcast_to(spins, (alpha.shape[0], spins.shape[0], spins.shape[1])), end_dim=-2)\n",
    "alpha_batched = torch.flatten(torch.broadcast_to(alpha.unsqueeze(1), (alpha.shape[0], spins.shape[0], alpha.shape[1])), end_dim=-2)\n",
    "print(alpha)\n",
    "print(spins)\n",
    "\n",
    "print(spins_batched)\n",
    "print(alpha_batched)\n",
    "#torch.cat((alpha, spins), dim=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4, 8],\n",
      "        [4, 8],\n",
      "        [1, 2],\n",
      "        [1, 2]])\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[4, 8],\n",
      "        [4, 8],\n",
      "        [4, 8],\n",
      "        [4, 8],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "a = torch.arange(0,8).reshape(2,2,2)\n",
    "print(a)\n",
    "a = torch.flatten(a, end_dim=-2)\n",
    "print(a)\n",
    "a = a.reshape(2,2,2)\n",
    "print(a)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0, 1],\n",
      "         [2, 3]],\n",
      "\n",
      "        [[4, 5],\n",
      "         [6, 7]]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n",
      "tensor([[[0, 1],\n",
      "         [2, 3]],\n",
      "\n",
      "        [[4, 5],\n",
      "         [6, 7]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}