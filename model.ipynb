{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import Operator as op\n",
    "import utils\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding to sequence\n",
      "Hamiltonan = Sx_2 + Sz_1 * Sy_2\n"
     ]
    }
   ],
   "source": [
    "lattice_sites = 3\n",
    "hamiltonian = []\n",
    "h = -1\n",
    "'''\n",
    "for l in range(lattice_sites):\n",
    "  hamiltonian = Sx(l)* (h) + Sz(l) * Sz((l+1) % lattice_sites) + hamiltonian\n",
    "print_op_list(hamiltonian)\n",
    "'''\n",
    "hamiltonian = op.Sx(2) + ([op.Sz(1)*op.Sy(2)] + hamiltonian)\n",
    "op.print_op_list(hamiltonian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer_1): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (layer_3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.lattice_struct = nn.Sequential(\n",
    "      nn.Conv1d(1, 8, 2),\n",
    "      nn.ReLu(),\n",
    "      nn.Conv1d(8, 16),\n",
    "      nn.Flatten()\n",
    "    )\n",
    "    \n",
    "    self.tNN = nn.Sequential(\n",
    "      nn.Linear(2, 16),\n",
    "      nn.ReLu(),\n",
    "      nn.Linear(16, 32),\n",
    "      nn.ReLu(),\n",
    "      nn.Linear(32, 64)\n",
    "    )\n",
    "\n",
    "    self.output = nn.Sequential(\n",
    "      nn.Linear( 64 + )\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.layer_2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.layer_3(x)\n",
    "    return x\n",
    "\n",
    "model = Model().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map:  tensor([[[ 1,  1, -1],\n",
      "         [ 1,  1, -1]]], device='cuda:0', dtype=torch.int8)\n",
      "mat els:  tensor([[[[ 1.+0.j,  1.+0.j,  1.+0.j],\n",
      "          [ 1.+0.j,  1.+0.j,  0.+1.j]],\n",
      "\n",
      "         [[ 1.+0.j,  1.+0.j,  1.+0.j],\n",
      "          [ 1.+0.j, -1.+0.j, -0.-1.j]]]], device='cuda:0')\n",
      "spin config:  tensor([[1., 1., 1.]], device='cuda:0')\n",
      "torch.Size([1, 2]) torch.Size([1, 1]) torch.Size([1, 3])\n",
      "time to calculate O_loc: 4.15e-02\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "map = utils.get_map(hamiltonian, lattice_sites)\n",
    "map = map.to(device)\n",
    "print(\"map: \", map)\n",
    "\n",
    "mat_els = utils.get_total_mat_els(hamiltonian, lattice_sites)\n",
    "mat_els = mat_els.to(device)\n",
    "print(\"mat els: \", mat_els)\n",
    "\n",
    "#1.dim: Batch\n",
    "#2.dim: lattice sites\n",
    "#s_config = get_all_spin_configs(lattice_sites)\n",
    "s_config = torch.tensor([1,1,1]).reshape(1,3)\n",
    "s_config = (s_config.type(torch.float32)).to(device)\n",
    "print(\"spin config: \", s_config)\n",
    "\n",
    "start = timer()\n",
    "s_p = utils.get_sp(s_config, map)\n",
    "psi_s = model(s_config)\n",
    "psi_sp = model(s_p.reshape(-1, lattice_sites)).reshape(s_p.shape[0], s_p.shape[1])\n",
    "print(psi_sp.shape, psi_s.shape, s_config.shape)\n",
    "O_loc = utils.calc_Oloc(psi_sp, mat_els, s_config)\n",
    "end = timer()\n",
    "\n",
    "print(f\"time to calculate O_loc: {end - start:.2e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8,  9, 10]],\n",
      "\n",
      "        [[11, 12, 13, 14, 15]]])\n",
      "tensor([[[ 5,  1,  2,  3,  4,  5,  1]],\n",
      "\n",
      "        [[10,  6,  7,  8,  9, 10,  6]],\n",
      "\n",
      "        [[15, 11, 12, 13, 14, 15, 11]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b3612a7abdb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    294\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 295\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "input = torch.arange(1,16,1)\n",
    "input = input.reshape(3,1,5)\n",
    "print(input)\n",
    "padded_input = F.pad(input, (1,1), mode='circular')\n",
    "print(padded_input)\n",
    "#model = nn.Conv1d(1, 1, 2)\n",
    "#model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}